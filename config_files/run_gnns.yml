experiment:
  # dataset:
  #   name: ml-100k
  #   item: # infos related to item dataset (mandatory, at least item_id)
  #     path: datasets/ml-100k/processed/item.csv
  #     extra_features: [movie_year, movie_title] # features(columns) beside item_id to be used
  #   user: # mandatory (at least user_id)
  #     path: datasets/ml-100k/processed/user.csv
  #     extra_features: [gender, occupation] # features beside user_id
  #   ratings: # mandatory (at least [user_id, item_id, rating])
  #     path: datasets/ml-100k/processed/rating.csv
  #     timestamp: True
  #   enrich:
  #     map_path: datasets/ml-100k/processed/map.csv
  #     enrich_path: datasets/ml-100k/processed/enriched.csv
  #     remove_unmatched: False
  #     properties: [subject, director, abstract]
  # dataset:
  #   name: lastfm
  #   item: # infos related to item dataset (mandatory, at least item_id)
  #     path: datasets/lastfm/processed/item.csv
  #     extra_features: [name] # features(columns) beside item_id to be used
  #   user: # mandatory (at least user_id)
  #     path: datasets/lastfm/processed/user.csv
  #   ratings: # mandatory (at least [user_id, item_id, rating])
  #     path: datasets/lastfm/processed/rating.csv
  #     timestamp: False
  #   social:
  #     path: datasets/lastfm/processed/social.csv
  #   enrich:
  #     map_path: datasets/lastfm/processed/map.csv
  #     enrich_path: datasets/lastfm/processed/enriched.csv
  #     remove_unmatched: False
  #     properties: [abstract, bandMember, genre, associatedMusicalArtist, awards, recordLabel, associatedBand, origin]
  # dataset:
  #   name: douban-movie
  #   item: # infos related to item dataset (mandatory, at least item_id)
  #     path: datasets/douban-movie/processed/item.csv
  #     extra_features: [name_EN, name_CN] # features(columns) beside item_id to be used
  #   user: # mandatory (at least user_id)
  #     path: datasets/douban-movie/processed/user.csv
  #     extra_features: [name]
  #   ratings: # mandatory (at least [user_id, item_id, rating])
  #     path: datasets/douban-movie/processed/rating.csv
  #     timestamp: False
  dataset:
    name: ml-1m
    item: # infos related to item dataset (mandatory, at least item_id)
      path: datasets/ml-1m/processed/item.csv
      extra_features: [movie_year, movie_title] # features(columns) beside item_id to be used
    user: # mandatory (at least user_id)
      path: datasets/ml-1m/processed/user.csv
      extra_features: [gender, occupation] # features beside user_id
    ratings: # mandatory (at least [user_id, item_id, rating])
      path: datasets/ml-1m/processed/rating.csv
      timestamp: True
    enrich:
      map_path: datasets/ml-1m/processed/map.csv
      enrich_path: datasets/ml-1m/processed/enriched.csv
      remove_unmatched: False
      properties: [subject, director, abstract]

  preprocess:
    # - method: filter_by_rating
    #   parameters:
    #     threshold: 20
    # - method: binarize
    #   parameters:
    #     threshold: 4
    - method: filter_kcore
      parameters:
        k: 20
        iterations: 1
        target: user # user or rating

  split:
    seed: 42
    # test:
    #   method: random_by_ratio
    #   level: global
    #   p: 0.2
    # validation:
    #   method: random_by_ratio
    #   level: global
    #   p: 0.2

    # test:
    #   method: timestamp_by_ratio
    #   level: user
    #   p: 0.1
    # validation:
    #   level: user
    #   method: timestamp_by_ratio
    #   p: 0.2

    # test:
    #   method: fixed_timestamp
    #   # type: global_level
    #   timestamp: 890000000
    # validation:
    #   method: fixed_timestamp
    #   timestamp: 880000000

    test:
      method: k_fold
      k: 5
      level: "user"

  models:
    # - name: bPRMF
    #   config:
    #     save_weights: True
    #   parameters:
    #     embed_size: 64
    #     epoch: 1000
    #     verbose: 10
    #     validate_factor: 200 # for ml-1m 200 50 for others
    #     validate_frac: .0001  # for ml-1m .0001 .001 for others
    #     test_flag: sort
    # - name: cFKG
    #   config:
    #     save_weights: True
    #   parameters:
    #     embed_size: 64
    #     epoch: 1000
    #     verbose: 10
    #     validate_factor: 200 # for ml-1m 200
    #     validate_frac: .0001  # for ml-1m .0001
    #     test_flag: sort
    - name: cKE
      config:
        save_weights: True
      parameters:
        embed_size: 64
        epoch: 1000
        verbose: 10
        validate_factor: 200 # for ml-1m 200
        validate_frac: .0001  # for ml-1m .0001
        test_flag: sort
    # - name: kGAT
    #   config:
    #     save_weights: True
    #   parameters:
    #     embed_size: 64
    #     epoch: 1000
    #     verbose: 10
    #     validate_factor: 200 # for ml-1m 200
    #     validate_frac: .0001  # for ml-1m .0001
    #     test_flag: sort

  evaluation:
    k: 10
    relevance_threshold: 0
    metrics: [MAP, nDCG]

  report:
    file: "experiment_results/fixed_db16_runs/ml-1m_enriched_cKE.csv"
    execution_times:
      file: "experiment_results/fixed_db16_runs/ml-1m_enriched_cKE_times.csv"